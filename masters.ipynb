{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "splits = {'train': 'plain_text/train-00000-of-00001.parquet', 'test': 'plain_text/test-00000-of-00001.parquet', 'unsupervised': 'plain_text/unsupervised-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitutes to 2nd algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = {\n",
    "    'a': '4', 'b': '6', 'i': '1', 'l': '1', 'o': '0', 's': '5', 'z': '2', 'e': '3','g':'9'\n",
    "}\n",
    "\n",
    "def format_text(df, image_size, letter_size):\n",
    "    def cut_text(text):\n",
    "        substituted_text = ''.join([subs.get(char, char) for char in text.lower()])\n",
    "        cut_text = ''.join([char for char in substituted_text if char in 'cdfhjkmnprtuvwxy 0123456789'])\n",
    "        max_length = (image_size[0] // letter_size[0]) * (image_size[1] // letter_size[1]) * 3\n",
    "        return cut_text[:max_length]\n",
    "    df['cut_text'] = df['text'].apply(cut_text)\n",
    "    return df\n",
    "\n",
    "df = format_text(df,(64,64),(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitution to 1 and 3 algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cut_text_column(df, image_size, letter_size, allowed_characters=None):\n",
    "    if allowed_characters is None:\n",
    "        allowed_characters = ''.join(chr(i) for i in range(min(255, image_size[0]*image_size[1]/letter_size[0]/letter_size[1])))\n",
    "    \n",
    "    def cut_text(text):\n",
    "        cut_text = ''.join([char for char in text.lower() if char in allowed_characters])\n",
    "        max_length = (image_size[0] // letter_size[0]) * (image_size[1] // letter_size[1]) * 3\n",
    "        return cut_text[:max_length]\n",
    "\n",
    "    df['cut_text'] = df['text'].apply(cut_text)\n",
    "    return df\n",
    "\n",
    "df = format_text(df,(64,64),(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General code to decode and encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def text_to_rgb_image_ascii(text, image_size=(64, 64), letter_size=(8, 8), allowed_chars=None):\n",
    "    image = Image.new('RGB', image_size, (255, 255, 255))\n",
    "    pixels = image.load()\n",
    "\n",
    "    if allowed_chars is None:\n",
    "        allowed_chars = ''.join(chr(i) for i in range(256))\n",
    "\n",
    "    text = ''.join([char for char in text if char in allowed_chars])\n",
    "\n",
    "    max_chars = (image_size[0] // letter_size[0]) * (image_size[1] // letter_size[1])\n",
    "\n",
    "    if len(text) > max_chars:\n",
    "        text = text[:max_chars]\n",
    "    else:\n",
    "        text = text.ljust(max_chars)\n",
    "\n",
    "    for i, char in enumerate(text):\n",
    "        ascii_value = ord(char)\n",
    "        row = (i // (image_size[0] // letter_size[0])) * letter_size[1]\n",
    "        col = (i % (image_size[0] // letter_size[0])) * letter_size[0]\n",
    "\n",
    "        for y in range(row, row + letter_size[1]):\n",
    "            for x in range(col, col + letter_size[0]):\n",
    "                if 48 <= ascii_value <= 57:\n",
    "                    pixels[x, y] = (255, 255, 255)\n",
    "                else:\n",
    "                    r = (ascii_value & 0xE0)\n",
    "                    g = (ascii_value & 0x1C) << 3\n",
    "                    b = (ascii_value & 0x03) << 6\n",
    "                    pixels[x, y] = (r, g, b)\n",
    "\n",
    "    return image, text\n",
    "\n",
    "def rgb_image_to_text_ascii(image, letter_size=(8, 8), allowed_chars=None):\n",
    "    if allowed_chars is None:\n",
    "        allowed_chars = ''.join(chr(i) for i in range(256))\n",
    "\n",
    "    image_width, image_height = image.size\n",
    "    num_chars_per_row = image_width // letter_size[0]\n",
    "    num_rows = image_height // letter_size[1]\n",
    "    max_chars = num_chars_per_row * num_rows\n",
    "\n",
    "    decoded_text = []\n",
    "\n",
    "    for i in range(max_chars):\n",
    "        row = (i // num_chars_per_row) * letter_size[1]\n",
    "        col = (i % num_chars_per_row) * letter_size[0]\n",
    "\n",
    "        r_total, g_total, b_total = 0, 0, 0\n",
    "        pixel_count = letter_size[0] * letter_size[1]\n",
    "\n",
    "        for y in range(row, row + letter_size[1]):\n",
    "            for x in range(col, col + letter_size[0]):\n",
    "                r, g, b = image.getpixel((x, y))\n",
    "                r_total += r\n",
    "                g_total += g\n",
    "                b_total += b\n",
    "\n",
    "        r_avg = r_total // pixel_count\n",
    "        g_avg = g_total // pixel_count\n",
    "        b_avg = b_total // pixel_count\n",
    "\n",
    "        ascii_value = (r_avg & 0xE0) | ((g_avg >> 3) & 0x1C) | ((b_avg >> 6) & 0x03)\n",
    "\n",
    "        if 0 <= ascii_value < 256 and chr(ascii_value) in allowed_chars:\n",
    "            decoded_text.append(chr(ascii_value))\n",
    "\n",
    "    return ''.join(decoded_text)\n",
    "\n",
    "test_text = 'Hello, World! This is a test for encoding ASCII text into an image.'\n",
    "\n",
    "test_image, filtered_text = text_to_rgb_image_ascii(\n",
    "    test_text, image_size=(64, 64), letter_size=(8, 8)\n",
    ")\n",
    "\n",
    "decoded_text = rgb_image_to_text_ascii(test_image, letter_size=(8, 8))\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(filtered_text)\n",
    "print(\"\\nDecoded Text:\")\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode and decode to way 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def text_to_rgb_image(text, image_size=(64, 64), letter_size=(8, 8)):\n",
    "\n",
    "    image = Image.new('RGB', image_size, (255, 255, 255))\n",
    "    pixels = image.load()\n",
    "\n",
    "\n",
    "    characters = 'abcdefghijklmnopqrstuvwxyz 0123456789'\n",
    "\n",
    "\n",
    "    char_to_gray = {char: int((i / (len(characters) - 1)) * 255) for i, char in enumerate(characters)}\n",
    "\n",
    "\n",
    "    max_chars = (image_size[0] // letter_size[0]) * (image_size[1] // letter_size[1])*3\n",
    "\n",
    "\n",
    "    text = ''.join([char for char in text.lower() if char in characters])[:max_chars]\n",
    "\n",
    "\n",
    "    chars_per_channel = max_chars // 3\n",
    "\n",
    "\n",
    "    for i, char in enumerate(text):\n",
    "        if char in char_to_gray:\n",
    "            grayscale_value = char_to_gray[char]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        ic = i%64\n",
    "        row = (ic // (image_size[0] // letter_size[0])) * letter_size[1]\n",
    "        col = (ic % (image_size[0] // letter_size[0])) * letter_size[0]\n",
    "\n",
    "\n",
    "        if i < chars_per_channel: \n",
    "            for y in range(row, row + letter_size[1]):\n",
    "                for x in range(col, col + letter_size[0]):\n",
    "                    r, g, b = pixels[x, y]\n",
    "                    pixels[x, y] = (grayscale_value, g, b)\n",
    "        elif i < 2 * chars_per_channel: \n",
    "            for y in range(row, row + letter_size[1]):\n",
    "                for x in range(col, col + letter_size[0]):\n",
    "                    r, g, b = pixels[x, y]\n",
    "                    pixels[x, y] = (r, grayscale_value, b)\n",
    "        else: \n",
    "            for y in range(row, row + letter_size[1]):\n",
    "                for x in range(col, col + letter_size[0]):\n",
    "                    r, g, b = pixels[x, y]\n",
    "                    pixels[x, y] = (r, g, grayscale_value)\n",
    "\n",
    "    return image, text\n",
    "\n",
    "\n",
    "\n",
    "def rgb_image_to_text(image, image_size=(64, 64), letter_size=(8, 8)):\n",
    "\n",
    "    characters = 'abcdefghijklmnopqrstuvwxyz 0123456789'\n",
    "    \n",
    "\n",
    "    char_to_gray = {char: int((i / (len(characters) - 1)) * 255) for i, char in enumerate(characters)}\n",
    "\n",
    "    sorted_chars = sorted(char_to_gray.items(), key=lambda item: item[1])\n",
    "    gray_values = [v for k, v in sorted_chars]\n",
    "    gray_to_char = {v: k for k, v in sorted_chars}\n",
    "    \n",
    "\n",
    "    def closest_char(gray):\n",
    "        closest = min(gray_values, key=lambda x: abs(x - gray))\n",
    "        return gray_to_char.get(closest, '?')  \n",
    "    \n",
    "\n",
    "    pixels = image.load()\n",
    "    \n",
    "    width, height = image_size\n",
    "    lw, lh = letter_size\n",
    "    cols = width // lw\n",
    "    rows = height // lh\n",
    "    chars_per_channel = cols * rows\n",
    "    \n",
    "    decoded_chars = {'R': [], 'G': [], 'B': []}\n",
    "    \n",
    "    for channel in ['R', 'G', 'B']:\n",
    "        for i in range(chars_per_channel):\n",
    "\n",
    "            row = (i // cols) * lh\n",
    "            col = (i % cols) * lw\n",
    "            \n",
    "\n",
    "            channel_values = []\n",
    "            for y in range(row, row + lh):\n",
    "                for x in range(col, col + lw):\n",
    "                    r, g, b = pixels[x, y]\n",
    "                    if channel == 'R':\n",
    "                        channel_values.append(r)\n",
    "                    elif channel == 'G':\n",
    "                        channel_values.append(g)\n",
    "                    elif channel == 'B':\n",
    "                        channel_values.append(b)\n",
    "            \n",
    "\n",
    "            avg_gray = sum(channel_values) / len(channel_values)\n",
    "            avg_gray = int(round(avg_gray))\n",
    "            \n",
    "\n",
    "            char = closest_char(avg_gray)\n",
    "            if avg_gray == 255:\n",
    "\n",
    "                continue\n",
    "            decoded_chars[channel].append(char)\n",
    "    \n",
    "\n",
    "    text = ''.join(decoded_chars['R'] + decoded_chars['G'] + decoded_chars['B'])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode and decode to way 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def text_to_rgb_image_mapped(text, image_size=(64, 64), letter_size=(8, 8)):\n",
    "    image = Image.new('RGB', image_size, (255, 255, 255))\n",
    "    pixels = image.load()\n",
    "    characters = 'cdfhjkmnprtuvwxy 0123456789RB'\n",
    "    characters_list = list(characters)\n",
    "    map_1_chars = characters_list[:10]    \n",
    "    map_2_chars = characters_list[10:20] \n",
    "    map_3_chars = characters_list[20:]\n",
    "\n",
    "    def create_char_to_gray_map(char_map):\n",
    "        return {char: int((i / (len(char_map) - 1)) * 255) for i, char in enumerate(char_map)}\n",
    "\n",
    "    char_to_gray_map1 = create_char_to_gray_map(map_1_chars)\n",
    "    char_to_gray_map2 = create_char_to_gray_map(map_2_chars)\n",
    "    char_to_gray_map3 = create_char_to_gray_map(map_3_chars)\n",
    "\n",
    "\n",
    "    max_chars = (image_size[0] // letter_size[0]) * (image_size[1] // letter_size[1])\n",
    "\n",
    "\n",
    "    filtered_text = ''.join([char for char in text.lower() if char in characters_list])[:max_chars]\n",
    "\n",
    "    i = 0  \n",
    "    while i < len(filtered_text):\n",
    "        char = filtered_text[i]\n",
    "\n",
    "        row = (i // (image_size[0] // letter_size[0])) * letter_size[1]\n",
    "        col = (i % (image_size[0] // letter_size[0])) * letter_size[0]\n",
    "\n",
    "        if char in map_1_chars:\n",
    "            grayscale_value = char_to_gray_map1[char]\n",
    "            for y in range(row, row + letter_size[1]):\n",
    "                for x in range(col, col + letter_size[0]):\n",
    "                    _, g, b = pixels[x, y]\n",
    "                    pixels[x, y] = (grayscale_value, g, 226)\n",
    "\n",
    "\n",
    "        elif char in map_2_chars:\n",
    "            grayscale_value = char_to_gray_map2[char]\n",
    "            for y in range(row, row + letter_size[1]):\n",
    "                for x in range(col, col + letter_size[0]):\n",
    "                    r, _, b = pixels[x, y]\n",
    "                    pixels[x, y] = (r, grayscale_value, 255)\n",
    "\n",
    "\n",
    "        elif char in map_3_chars:\n",
    "            grayscale_value = char_to_gray_map3[char]\n",
    "            for y in range(row, row + letter_size[1]):\n",
    "                for x in range(col, col + letter_size[0]):\n",
    "                    r, g, b = pixels[x, y]\n",
    "                    pixels[x, y] = (r, g, grayscale_value)\n",
    "        i += 1 \n",
    "    return image, filtered_text\n",
    "\n",
    "def rgb_image_to_text(image, image_size=(64, 64), letter_size=(8, 8)):\n",
    "    characters = 'cdfhjkmnprtuvwxy 0123456789'\n",
    "    gray_to_char = {int((i / (len(characters) - 1)) * 255): char for i, char in enumerate(characters)}\n",
    "    pixels = image.load()\n",
    "    max_chars = (image_size[0] // letter_size[0]) * (image_size[1] // letter_size[1]) * 3\n",
    "    chars_per_channel = max_chars // 3\n",
    "    decoded_text = ''\n",
    "    for i in range(max_chars):\n",
    "        ic = i % 64\n",
    "        row_start = (ic // (image_size[0] // letter_size[0])) * letter_size[1]\n",
    "        col_start = (ic % (image_size[0] // letter_size[0])) * letter_size[0]\n",
    "        r_sum, g_sum, b_sum = 0, 0, 0\n",
    "        total_pixels = letter_size[0] * letter_size[1]\n",
    "        for row_offset in range(letter_size[1]):\n",
    "            for col_offset in range(letter_size[0]):\n",
    "                r, g, b = pixels[col_start + col_offset, row_start + row_offset]\n",
    "                r_sum += r\n",
    "                g_sum += g\n",
    "                b_sum += b\n",
    "        avg_r = r_sum // total_pixels\n",
    "        avg_g = g_sum // total_pixels\n",
    "        avg_b = b_sum // total_pixels\n",
    "        if i < chars_per_channel: \n",
    "            grayscale_value = avg_r\n",
    "        elif i < 2 * chars_per_channel: \n",
    "            grayscale_value = avg_g\n",
    "        else:  \n",
    "            grayscale_value = avg_b\n",
    "        closest_gray_value = min(gray_to_char.keys(), key=lambda x: abs(x - grayscale_value))\n",
    "        decoded_char = gray_to_char[closest_gray_value]\n",
    "        decoded_text += decoded_char\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:12800]\n",
    "df = create_cut_text_column(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train_images = 12800\n",
    "train_size = int(0.7 * sub_train_images)\n",
    "val_size = int(0.1 * sub_train_images)\n",
    "test_size = sub_train_images - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextImageDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.sample(frac=1).reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.image_size = (64,64)\n",
    "        self.letter_size = (4,4)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.df.iloc[idx]['image']\n",
    "        text = self.df.iloc[idx]['cut_text']  \n",
    "\n",
    "\n",
    "        if isinstance(image, Image.Image): \n",
    "            if self.transform:\n",
    "                image = self.transform(image)  \n",
    "\n",
    "        return image, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)), \n",
    "    transforms.ToTensor(),             \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n",
    "])\n",
    "\n",
    "\n",
    "text_image_dataset_train = TextImageDataset(train_text_df, transform=image_transforms)\n",
    "text_image_dataset_val = TextImageDataset(val_text_df, transform=image_transforms)\n",
    "text_image_dataset_test = TextImageDataset(test_text_df, transform=image_transforms)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "text_image_train_dataloader = DataLoader(text_image_dataset_train, batch_size=64, shuffle=True, num_workers=4)\n",
    "text_image_val_dataloader = DataLoader(text_image_dataset_val, batch_size=64, shuffle=True, num_workers=4)\n",
    "text_image_test_dataloader = DataLoader(text_image_dataset_test, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "cropH, cropW = 64, 64\n",
    "batchSize = 64\n",
    "dataDir = f\"{nikhilshingadiya_tinyimagenet200_path}/tiny-imagenet-200\"\n",
    "trainDir = f\"{dataDir}/train\"\n",
    "sub_size = 12800  # Select a subset of images\n",
    "\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop((cropH, cropW), pad_if_needed=True),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "train_images = datasets.ImageFolder(trainDir, transform=data_transforms)\n",
    "\n",
    "\n",
    "indices_train = np.random.choice(len(train_images), size=sub_size, replace=False)\n",
    "sub_train_images = torch.utils.data.Subset(train_images, indices_train)\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(sub_train_images, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batchSize, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchSize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CBAMBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(CBAMBlock, self).__init__()\n",
    "        # Channel Attention\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // reduction, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # Spatial Attention\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, kernel_size=7, padding=3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Channel Attention\n",
    "        ca = self.channel_attention(x)\n",
    "        x = x * ca\n",
    "        # Spatial Attention\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        sa_input = torch.cat([avg_out, max_out], dim=1)\n",
    "        sa = self.spatial_attention(sa_input)\n",
    "        x = x * sa\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualCBAMBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualCBAMBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels)\n",
    "        )\n",
    "        self.cbam = CBAMBlock(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.conv(x)\n",
    "        x = self.cbam(x)\n",
    "        x += res\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Initial convolution\n",
    "        self.conv_in = nn.Sequential(\n",
    "            nn.Conv2d(6, 64, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Residual CBAM blocks\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResidualCBAMBlock(64),\n",
    "            ResidualCBAMBlock(64),\n",
    "            ResidualCBAMBlock(64),\n",
    "            ResidualCBAMBlock(64),\n",
    "            ResidualCBAMBlock(64),\n",
    "            ResidualCBAMBlock(64),\n",
    "            ResidualCBAMBlock(64),\n",
    "        )\n",
    "        # Output convolution\n",
    "        self.conv_out = nn.Sequential(\n",
    "            nn.Conv2d(64, 3, kernel_size=7, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_S, input_C):\n",
    "        # Concatenate input images\n",
    "        x = torch.cat([input_S, input_C], dim=1)\n",
    "        x = self.conv_in(x)\n",
    "        x = self.res_blocks(x)\n",
    "        output = self.conv_out(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Initial convolution\n",
    "        self.conv_in = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Residual CBAM blocks\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResidualCBAMBlock(64),\n",
    "            ResidualCBAMBlock(64),\n",
    "            ResidualCBAMBlock(64),\n",
    "            ResidualCBAMBlock(64),\n",
    "            ResidualCBAMBlock(64),\n",
    "            ResidualCBAMBlock(64),\n",
    "            ResidualCBAMBlock(64)\n",
    "\n",
    "        )\n",
    "\n",
    "        self.conv_out = nn.Sequential(\n",
    "            nn.Conv2d(64, 3, kernel_size=7, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_in(x)\n",
    "        x = self.res_blocks(x)\n",
    "        output = self.conv_out(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, input_S, input_C):\n",
    "        output_Cprime = self.encoder(input_S, input_C)\n",
    "        output_Sprime = self.decoder(output_Cprime)\n",
    "        return output_Cprime, output_Sprime\n",
    "\n",
    "    def pixel_errors(self, input_S, input_C):\n",
    "        with torch.no_grad():\n",
    "            output_Cprime, output_Sprime = self.forward(input_S, input_C)\n",
    "            diff_C = torch.abs(output_Cprime - input_C)\n",
    "            diff_S = torch.abs(output_Sprime - input_S)\n",
    "            see_Cpixel = torch.sqrt(torch.mean(diff_C ** 2)).item()\n",
    "            see_Spixel = torch.sqrt(torch.mean(diff_S ** 2)).item()\n",
    "        return see_Cpixel, see_Spixel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, text_image_train_loader, text_image_val_loader, num_epochs=1):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(\"device : \", device)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    S_mseloss = torch.nn.MSELoss().to(device) \n",
    "    C_mseloss = torch.nn.MSELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "    loss_all_total, c_loss_total, s_loss_total = [], [], []\n",
    "    val_loss_all_total, val_c_loss_total, val_s_loss_total = [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()  \n",
    "        loss_all, c_loss, s_loss = [], [], []\n",
    "\n",
    "        train_iter = iter(train_loader)\n",
    "        text_train_iter = iter(text_image_train_loader)  \n",
    "        t = tqdm(range(len(train_loader)), desc=f\"Epoch {epoch+1} [Training]\")\n",
    "\n",
    "        for _ in t:\n",
    "            try:\n",
    "                cover_images, _ = next(train_iter)\n",
    "                cover_images = cover_images.to(device)\n",
    "                image_secrets, _ = next(text_train_iter)\n",
    "                image_secrets = image_secrets.to(device)\n",
    "\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "\n",
    "            output_C, output_S = model(image_secrets, cover_images)\n",
    "\n",
    "\n",
    "            beta = 1.0\n",
    "            ssLoss = S_mseloss(image_secrets, output_S)\n",
    "            ccLoss = C_mseloss(cover_images, output_C)\n",
    "            loss = beta * ssLoss + ccLoss\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            loss_all.append(loss.item())\n",
    "            c_loss.append(ccLoss.item())\n",
    "            s_loss.append(ssLoss.item())\n",
    "\n",
    "\n",
    "            t.set_description(f\"Epoch {epoch+1} [Training] | Loss: {np.mean(loss_all):.4f}\")\n",
    "\n",
    "\n",
    "        loss_all_total.append(np.mean(loss_all))\n",
    "        c_loss_total.append(np.mean(c_loss))\n",
    "        s_loss_total.append(np.mean(s_loss))\n",
    "\n",
    "\n",
    "        model.eval()  \n",
    "        val_loss_all, val_c_loss, val_s_loss = [], [], []\n",
    "\n",
    "        val_iter = iter(val_loader)\n",
    "        text_val_iter = iter(text_image_val_loader)  \n",
    "        v = tqdm(range(len(val_loader)), desc=f\"Epoch {epoch+1} [Validation]\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in v:\n",
    "                try:\n",
    "                    cover_images, _ = next(val_iter)\n",
    "                    cover_images = cover_images.to(device)\n",
    "                    image_secrets, _ = next(text_val_iter)\n",
    "                    image_secrets = image_secrets.to(device)\n",
    "\n",
    "                except StopIteration:\n",
    "                    break\n",
    "\n",
    "                output_C, output_S = model(image_secrets, cover_images)\n",
    "                ssLoss = S_mseloss(image_secrets, output_S)\n",
    "                ccLoss = C_mseloss(cover_images, output_C)\n",
    "                val_loss = beta * ssLoss + ccLoss\n",
    "                val_loss_all.append(val_loss.item())\n",
    "                val_c_loss.append(ccLoss.item())\n",
    "                val_s_loss.append(ssLoss.item())\n",
    "                v.set_description(f\"Epoch {epoch+1} [Validation] | Loss: {np.mean(val_loss_all):.4f}\")\n",
    "\n",
    "\n",
    "        val_loss_all_total.append(np.mean(val_loss_all))\n",
    "        val_c_loss_total.append(np.mean(val_c_loss))\n",
    "        val_s_loss_total.append(np.mean(val_s_loss))\n",
    "\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Training Loss: {np.mean(loss_all)} | Cover Loss: {np.mean(c_loss)} | Secret Loss: {np.mean(s_loss)}\")\n",
    "        print(f\"[Epoch {epoch+1}] Validation Loss: {np.mean(val_loss_all)} | Cover Loss: {np.mean(val_c_loss)} | Secret Loss: {np.mean(val_s_loss)}\")\n",
    "\n",
    "    return model, (loss_all_total, c_loss_total, s_loss_total), (val_loss_all_total, val_c_loss_total, val_s_loss_total)\n",
    "\n",
    "model=Model()\n",
    "\n",
    "trained_model, train_history, val_history = train(model, train_loader, val_loader, text_image_train_dataloader, text_image_val_dataloader, num_epochs=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "train_total_loss = train_history[0]\n",
    "train_cover_loss = train_history[1]\n",
    "train_secret_loss = train_history[2]\n",
    "\n",
    "val_total_loss = val_history[0]\n",
    "val_cover_loss = val_history[1]\n",
    "val_secret_loss = val_history[2]\n",
    "\n",
    "epochs = range(1, 17)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(epochs, train_total_loss, 'g-', label='Train Total Loss')\n",
    "plt.plot(epochs, val_total_loss, 'b-', label='Val Total Loss')\n",
    "plt.title('Total Loss History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(epochs, train_cover_loss, 'g-', label='Train Cover Loss')\n",
    "plt.plot(epochs, val_cover_loss, 'b-', label='Val Cover Loss')\n",
    "plt.title('Cover Image Loss History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(epochs, train_secret_loss, 'g-', label='Train Secret Loss')\n",
    "plt.plot(epochs, val_secret_loss, 'b-', label='Val Secret Loss')\n",
    "plt.title('Secret Image Loss History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Demonstrating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def display_results(image_dataloader, text_image_dataloader, model):\n",
    "    model.eval()\n",
    "    image_data_iter = iter(image_dataloader)\n",
    "    text_data_iter = iter(text_image_dataloader)\n",
    "    cover_images, _ = next(image_data_iter) \n",
    "    image_secrets, _ = next(text_data_iter)   \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    image_secrets = image_secrets.to(device)\n",
    "    cover_images = cover_images.to(device)\n",
    "    with torch.no_grad():\n",
    "        output_covers, output_secrets = model(image_secrets, cover_images)\n",
    "    image_secrets = image_secrets.cpu().numpy()\n",
    "    cover_images = cover_images.cpu().numpy()\n",
    "    output_secrets = output_secrets.cpu().numpy()\n",
    "    output_covers = output_covers.cpu().numpy()\n",
    "    fig, axs = plt.subplots(4, 5, figsize=(15, 12)) \n",
    "    for i in range(5):\n",
    "        img = (image_secrets[i].transpose(1, 2, 0) + 1) / 2 \n",
    "        axs[0, i].imshow(img)\n",
    "        axs[0, i].set_title(f'Secret {i+1}')\n",
    "        axs[0, i].axis('off') \n",
    "    for i in range(5):\n",
    "        img = (output_secrets[i].transpose(1, 2, 0) + 1) / 2 \n",
    "        axs[1, i].imshow(img)\n",
    "        axs[1, i].set_title(f'Restored secret {i+1}')\n",
    "        axs[1, i].axis('off')\n",
    "    for i in range(5):\n",
    "        img = (cover_images[i].transpose(1, 2, 0) + 1) / 2  \n",
    "        axs[2, i].imshow(img)\n",
    "        axs[2, i].set_title(f'Cover {i+1}')\n",
    "        axs[2, i].axis('off') \n",
    "    for i in range(5):\n",
    "        img = (output_covers[i].transpose(1, 2, 0) + 1) / 2\n",
    "        axs[3, i].imshow(img)\n",
    "        axs[3, i].set_title(f'Cover with hidden image{i+1}')\n",
    "        axs[3, i].axis('off')  \n",
    "    plt.show()\n",
    "\n",
    "display_results(val_loader, text_image_val_dataloader, AEmodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Fast calculation of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "\n",
    "def rgb_image_to_text_with_average_fixed(image, letter_size=(8, 8)):\n",
    "    characters_list = [chr(i) for i in range(128)]\n",
    "    value_to_state = {0: 0, 51: 1, 102: 2, 153: 3, 204: 4, 255: 5}\n",
    "    possible_values = list(value_to_state.keys())\n",
    "\n",
    "    def round_to_nearest(value):\n",
    "        return min(possible_values, key=lambda x: abs(x - value))\n",
    "\n",
    "    def base7_rgb_to_position(r, g, b):\n",
    "        r_rounded = round_to_nearest(r)\n",
    "        g_rounded = round_to_nearest(g)\n",
    "        b_rounded = round_to_nearest(b)\n",
    "\n",
    "        r_state = value_to_state[r_rounded]\n",
    "        g_state = value_to_state[g_rounded]\n",
    "        b_state = value_to_state[b_rounded]\n",
    "        position = r_state + (g_state * 6) + (b_state * 36)  \n",
    "\n",
    "        return position\n",
    "\n",
    "    def average_rgb_in_block(x_start, y_start, width, height):\n",
    "        r_total, g_total, b_total = 0, 0, 0\n",
    "        pixel_count = width * height\n",
    "\n",
    "        for y in range(y_start, y_start + height):\n",
    "            for x in range(x_start, x_start + width):\n",
    "                r, g, b = image.getpixel((x, y))\n",
    "                r_total += r\n",
    "                g_total += g\n",
    "                b_total += b\n",
    "\n",
    "        r_avg = r_total // pixel_count\n",
    "        g_avg = g_total // pixel_count\n",
    "        b_avg = b_total // pixel_count\n",
    "\n",
    "        return r_avg, g_avg, b_avg\n",
    "\n",
    "    image_width, image_height = image.size\n",
    "    num_chars_per_row = image_width // letter_size[0]\n",
    "    num_rows = image_height // letter_size[1]\n",
    "    max_chars = num_chars_per_row * num_rows\n",
    "    decoded_text = []\n",
    "\n",
    "    for i in range(max_chars):\n",
    "        row = (i // num_chars_per_row) * letter_size[1]\n",
    "        col = (i % num_chars_per_row) * letter_size[0]\n",
    "\n",
    "        r_avg, g_avg, b_avg = average_rgb_in_block(col, row, letter_size[0], letter_size[1])\n",
    "\n",
    "        position = base7_rgb_to_position(r_avg, g_avg, b_avg)\n",
    "\n",
    "        if position < len(characters_list):\n",
    "            decoded_text.append(characters_list[position])\n",
    "        else:\n",
    "            decoded_text.append(' ')\n",
    "    \n",
    "    decoded_string = ''.join(decoded_text)\n",
    "    \n",
    "\n",
    "    word = ''\n",
    "    for char in decoded_string:\n",
    "        if not char.isalpha():\n",
    "            if word == \"\":\n",
    "                corrected_text+=char\n",
    "                continue\n",
    "            corrected_word = spell.correction(word)\n",
    "            if corrected_word is None:\n",
    "                corrected_text+=word\n",
    "                corrected_word = \"\"\n",
    "            elif len(word)==len(corrected_word):\n",
    "                new_word = \"\"\n",
    "                for i in range(len(word)):\n",
    "                    if word[i].lower() == corrected_word[i].lower():\n",
    "                        new_word+=word[i]\n",
    "                    else:\n",
    "                        new_word += corrected_word[i]\n",
    "                corrected_text+=new_word\n",
    "                \n",
    "            else:\n",
    "                corrected_text+=word\n",
    "            corrected_word = \"\"\n",
    "            word = \"\"\n",
    "            corrected_text+=char\n",
    "        else:\n",
    "            word+=char\n",
    "    if word!=\"\":\n",
    "        corrected_word = spell.correction(word)\n",
    "        if corrected_word is None:\n",
    "            corrected_text+=word\n",
    "            corrected_word = \"\"\n",
    "        elif len(word)==len(corrected_word):\n",
    "            new_word = \"\"\n",
    "            for i in range(len(word)):\n",
    "                if word[i].lower() == corrected_word[i].lower():\n",
    "                    new_word+=word[i]\n",
    "                else:\n",
    "                    new_word += corrected_word[i]\n",
    "            corrected_text+=new_word\n",
    "        else:\n",
    "            corrected_text+=word\n",
    "\n",
    "    return decoded_string, decoded_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation with dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def confusion_matric(restored_texts, ground_truth_texts):\n",
    "    characters = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .' \n",
    "    char_to_idx = {char: idx for idx, char in enumerate(characters)}\n",
    "    confusion_matrix = np.zeros((len(characters), len(characters)), dtype=int)\n",
    "\n",
    "    total_chars = 0\n",
    "    correct_chars = 0\n",
    "    exact_matches = 0\n",
    "\n",
    "    for restored, ground_truth in zip(restored_texts, ground_truth_texts):\n",
    "\n",
    "        restored = restored[:len(ground_truth)]\n",
    "        for r_char, gt_char in zip(restored, ground_truth):\n",
    "            total_chars += 1\n",
    "            if r_char == gt_char:\n",
    "                correct_chars += 1\n",
    "\n",
    "            if gt_char in char_to_idx and r_char in char_to_idx:\n",
    "                confusion_matrix[char_to_idx[gt_char], char_to_idx[r_char]] += 1\n",
    "\n",
    "\n",
    "        if restored == ground_truth:\n",
    "            exact_matches += 1\n",
    "\n",
    "    char_accuracy = (correct_chars / total_chars) * 100 if total_chars > 0 else 0\n",
    "    exact_match_accuracy = (exact_matches / len(ground_truth_texts)) * 100\n",
    "\n",
    "    return char_accuracy, exact_match_accuracy, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "\n",
    "def rgb_image_to_text(image, letter_size=(8, 8), N=6):\n",
    "    max_positions = N ** 3\n",
    "    characters_list = [chr(i) for i in range(max_positions)]\n",
    "    \n",
    "    spell = SpellChecker()\n",
    "    decoded_string = ''\n",
    "    corrected_text = ''\n",
    "    word = ''\n",
    "\n",
    "    value_to_state = {}\n",
    "    possible_values = []\n",
    "    if N > 1:\n",
    "        step = 255 // (N - 1)\n",
    "    else:\n",
    "        step = 255  \n",
    "\n",
    "    for i in range(N):\n",
    "        value = i * step\n",
    "        possible_values.append(value)\n",
    "        value_to_state[value] = i\n",
    "\n",
    "    def round_to_nearest(value):\n",
    "        return min(possible_values, key=lambda x: abs(x - value))\n",
    "\n",
    "    def baseN_rgb_to_position(r, g, b):\n",
    "        r_rounded = round_to_nearest(r)\n",
    "        g_rounded = round_to_nearest(g)\n",
    "        b_rounded = round_to_nearest(b)\n",
    "\n",
    "        r_state = value_to_state[r_rounded]\n",
    "        g_state = value_to_state[g_rounded]\n",
    "        b_state = value_to_state[b_rounded]\n",
    "\n",
    "        position = r_state + (g_state * N) + (b_state * N * N)\n",
    "        return position\n",
    "\n",
    "    def average_rgb_in_block(x_start, y_start, width, height):\n",
    "        r_total, g_total, b_total = 0, 0, 0\n",
    "        pixel_count = width * height\n",
    "\n",
    "        for y in range(y_start, y_start + height):\n",
    "            for x in range(x_start, x_start + width):\n",
    "                r, g, b = image.getpixel((x, y))\n",
    "                r_total += r\n",
    "                g_total += g\n",
    "                b_total += b\n",
    "\n",
    "        r_avg = r_total // pixel_count\n",
    "        g_avg = g_total // pixel_count\n",
    "        b_avg = b_total // pixel_count\n",
    "\n",
    "        return r_avg, g_avg, b_avg\n",
    "\n",
    "    image_width, image_height = image.size\n",
    "    num_chars_per_row = image_width // letter_size[0]\n",
    "    num_rows = image_height // letter_size[1]\n",
    "    max_chars = num_chars_per_row * num_rows\n",
    "\n",
    "    for i in range(max_chars):\n",
    "        row = (i // num_chars_per_row) * letter_size[1]\n",
    "        col = (i % num_chars_per_row) * letter_size[0]\n",
    "\n",
    "        r_avg, g_avg, b_avg = average_rgb_in_block(col, row, letter_size[0], letter_size[1])\n",
    "\n",
    "        position = baseN_rgb_to_position(r_avg, g_avg, b_avg)\n",
    "\n",
    "        if position < len(characters_list):\n",
    "            character = characters_list[position]\n",
    "        else:\n",
    "            character = ' ' \n",
    "\n",
    "        decoded_string += character\n",
    "\n",
    "        if not character.isalpha():\n",
    "            if word == \"\":\n",
    "                corrected_text += character\n",
    "                continue\n",
    "            corrected_word = spell.correction(word)\n",
    "            if corrected_word is None:\n",
    "                corrected_text += word\n",
    "            elif len(word) == len(corrected_word):\n",
    "                new_word = ''\n",
    "                for j in range(len(word)):\n",
    "                    if word[j].islower() == corrected_word[j].islower():\n",
    "                        new_word += word[j]\n",
    "                    else:\n",
    "                        new_word += corrected_word[j]\n",
    "                corrected_text += new_word\n",
    "            else:\n",
    "                corrected_text += word\n",
    "            word = ''\n",
    "            corrected_text += character\n",
    "        else:\n",
    "            word += character\n",
    "\n",
    "    if word != \"\":\n",
    "        corrected_word = spell.correction(word)\n",
    "        if corrected_word is None:\n",
    "            corrected_text += word\n",
    "        elif len(word) == len(corrected_word):\n",
    "            new_word = ''\n",
    "            for j in range(len(word)):\n",
    "                if word[j].islower() == corrected_word[j].islower():\n",
    "                    new_word += word[j]\n",
    "                else:\n",
    "                    new_word += corrected_word[j]\n",
    "            corrected_text += new_word\n",
    "        else:\n",
    "            corrected_text += word\n",
    "\n",
    "    return decoded_string, corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def evaluate_text_restoration_accuracy(image_dataloader, text_image_dataloader, model):\n",
    "    model.eval()\n",
    "\n",
    "    restored_texts = []\n",
    "    ground_truth_texts = []\n",
    "    decoded_texts = []\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    total_mse_loss = 0\n",
    "    total_ssim_score = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    error_counts_decoded = []\n",
    "    error_counts_restored = []\n",
    "\n",
    "    decoded_errors = []\n",
    "    restored_errors = []\n",
    "\n",
    "\n",
    "    for (cover_images, _), (secret_images, texts) in zip(image_dataloader, text_image_dataloader):\n",
    "        secret_images = secret_images.to(device)\n",
    "        cover_images = cover_images.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_covers, output_secrets = model(secret_images, cover_images)\n",
    "\n",
    "        output_covers = output_covers.to(device)\n",
    "\n",
    "        output_secrets = output_secrets.cpu().numpy()\n",
    "\n",
    "        mse_loss = F.mse_loss(output_covers, cover_images, reduction='mean')\n",
    "        total_mse_loss += mse_loss.item()\n",
    "\n",
    "        batch_ssim = ssim(output_covers, cover_images, data_range=1.0)\n",
    "        total_ssim_score += batch_ssim.item()\n",
    "\n",
    "        num_batches += 1\n",
    "\n",
    "        for i in range(output_secrets.shape[0]):\n",
    "            img = (output_secrets[i].transpose(1, 2, 0) + 1) / 2  \n",
    "            pil_image = Image.fromarray((img * 255).astype('uint8'))\n",
    "            pil_image.save(f'image{i}.png')\n",
    "            restored_text, decoded_text = rgb_image_to_text(pil_image) \n",
    "            restored_text = restored_text[:len(texts[i])]\n",
    "            decoded_text = decoded_text[:len(texts[i])]\n",
    "            decoded_texts.append(decoded_text)\n",
    "            restored_texts.append(restored_text)\n",
    "            ground_truth_texts.append(texts[i]) \n",
    "\n",
    "            decoded_error_count = sum(1 for a, b in zip(decoded_text, texts[i]) if a != b)\n",
    "            restored_error_count = sum(1 for a, b in zip(restored_text, texts[i]) if a != b)\n",
    "\n",
    "            decoded_errors.append(decoded_error_count)\n",
    "            restored_errors.append(restored_error_count)\n",
    "            error_counts_decoded.append(decoded_error_count)\n",
    "            error_counts_restored.append(restored_error_count)\n",
    "\n",
    "\n",
    "    char_accuracy, exact_match_accuracy, confusion_matrix = calculate_confusion_matrix_and_accuracy(\n",
    "        restored_texts, ground_truth_texts\n",
    "    )\n",
    "\n",
    "    avg_mse_loss = total_mse_loss / num_batches\n",
    "    avg_ssim_score = total_ssim_score / num_batches\n",
    "\n",
    "    print(f\"Character-Level Accuracy Decoded (Dataset): {char_accuracy:.2f}%\")\n",
    "    print(f\"Exact Match Accuracy Decoded (Dataset): {exact_match_accuracy:.2f}%\")\n",
    "    print(f\"Average MSE Loss (Cover Images): {avg_mse_loss:.4f}\")\n",
    "    print(f\"Average SSIM Score (Cover Images): {avg_ssim_score:.4f}\")\n",
    "\n",
    "    confusion_matrix_normalized = confusion_matrix / confusion_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "    characters = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .'\n",
    "    plt.figure(figsize=(24, 20))\n",
    "    sns.heatmap(confusion_matrix_normalized, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", xticklabels=characters, yticklabels=characters)\n",
    "    plt.title(\"Character-Level Reconstruction Confusion Matrix\")\n",
    "    plt.xlabel(\"Restored Characters\")\n",
    "    plt.ylabel(\"Ground Truth Characters\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    char_accuracy, exact_match_accuracy, confusion_matrix = calculate_confusion_matrix_and_accuracy(\n",
    "        decoded_texts, ground_truth_texts\n",
    "    )\n",
    "\n",
    "    avg_mse_loss = total_mse_loss / num_batches\n",
    "    avg_ssim_score = total_ssim_score / num_batches\n",
    "\n",
    "    print(f\"Character-Level Accuracy Decoded (Dataset): {char_accuracy:.2f}%\")\n",
    "    print(f\"Exact Match Accuracy Decoded (Dataset): {exact_match_accuracy:.2f}%\")\n",
    "\n",
    "    confusion_matrix_normalized = confusion_matrix / confusion_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "    characters = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .'\n",
    "    plt.figure(figsize=(24, 20))\n",
    "    sns.heatmap(confusion_matrix_normalized, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", xticklabels=characters, yticklabels=characters)\n",
    "    plt.title(\"Character-Level Reconstruction Confusion Matrix\")\n",
    "    plt.xlabel(\"Restored Characters\")\n",
    "    plt.ylabel(\"Ground Truth Characters\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    error_values, frequency_values = zip(*error_counts_decoded)\n",
    "    total_reconstructions = sum(frequency_values)\n",
    "    error_percentages = [freq / total_reconstructions * 100 for freq in frequency_values]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(error_values, error_percentages, color='skyblue')\n",
    "    plt.xlabel(\"Number of Errors\")\n",
    "    plt.ylabel(\"Percentage of Reconstructions in Decoded(%)\")\n",
    "    plt.title(\"Distribution of Errors in Text Reconstructions in Decoded\")\n",
    "    plt.xticks(range(max(error_values) + 1)) \n",
    "    plt.show()\n",
    "\n",
    "    error_values, frequency_values = zip(*error_counts_restored)\n",
    "    total_reconstructions = sum(frequency_values)\n",
    "    error_percentages = [freq / total_reconstructions * 100 for freq in frequency_values]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(error_values, error_percentages, color='skyblue')\n",
    "    plt.xlabel(\"Number of Errors\")\n",
    "    plt.ylabel(\"Percentage of Reconstructions in Corrected(%)\")\n",
    "    plt.title(\"Distribution of Errors in Text Reconstructions in Corrected\")\n",
    "    plt.xticks(range(max(error_values) + 1)) \n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
